{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-19T19:56:36.564458Z",
     "start_time": "2024-03-19T19:56:33.567472Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Documents\\xkocian\\VisionTrasnformer\\venv\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\Documents\\xkocian\\VisionTrasnformer\\venv\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.11.0 and strictly below 2.14.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version 2.10.0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from textwrap import wrap\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow_addons as tfa\n",
    "import os, warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "print('TensorFlow Version ' + tf.__version__)\n",
    "\n",
    "def seed_everything(seed = 0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "seed_everything()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "image_size = 224\n",
    "batch_size = 8\n",
    "n_classes = 3\n",
    "EPOCHS = 20\n",
    "\n",
    "train_path = 'test_smote_minority'\n",
    "\n",
    "classes = {0 : \"Dry\",\n",
    "           1 : \"Normal\",\n",
    "           2 : \"Wet\"}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T19:56:36.580417Z",
     "start_time": "2024-03-19T19:56:36.565432Z"
    }
   },
   "id": "68dd78c5c81853ee",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def data_augment(image):\n",
    "    p_spatial = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n",
    "    p_rotate = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n",
    "    p_pixel_1 = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n",
    "    p_pixel_3 = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n",
    "    \n",
    "    # Flips\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    \n",
    "    if p_spatial > .75:\n",
    "        image = tf.image.transpose(image)\n",
    "        \n",
    "    # Rotates\n",
    "    if p_rotate > .75:\n",
    "        image = tf.image.rot90(image, k = 3) # rotate 270ยบ\n",
    "    elif p_rotate > .5:\n",
    "        image = tf.image.rot90(image, k = 2) # rotate 180ยบ\n",
    "    elif p_rotate > .25:\n",
    "        image = tf.image.rot90(image, k = 1) # rotate 90ยบ\n",
    "        \n",
    "    # Pixel-level transforms\n",
    "    if p_pixel_1 >= .4:\n",
    "        image = tf.image.random_saturation(image, lower = .7, upper = 1.3)\n",
    "    if p_pixel_3 >= .4:\n",
    "        image = tf.image.random_brightness(image, max_delta = .1)\n",
    "        \n",
    "    return image"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T19:56:36.596349Z",
     "start_time": "2024-03-19T19:56:36.581388Z"
    }
   },
   "id": "a0a1ab26b537a7b8",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3607 images belonging to 3 classes.\n",
      "Found 1544 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255,\n",
    "                                                          samplewise_center = True,\n",
    "                                                          samplewise_std_normalization = True,\n",
    "                                                          validation_split = 0.3,\n",
    "                                                          dtype='float16'\n",
    "                                                        \n",
    "                                                          )\n",
    "# set as training data\n",
    "\n",
    "train_gen  = datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(image_size, image_size),\n",
    "    batch_size = batch_size,\n",
    "    seed = 1,\n",
    "    color_mode = 'rgb',\n",
    "    shuffle = True,\n",
    "    class_mode='categorical',\n",
    "    subset='training') \n",
    "\n",
    "# same directory as training data\n",
    "\n",
    "valid_gen  = datagen.flow_from_directory(\n",
    "    train_path ,\n",
    "    target_size=(image_size, image_size),\n",
    "    batch_size = batch_size,\n",
    "    seed = 1,\n",
    "    color_mode = 'rgb',\n",
    "    shuffle = False,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    )\n",
    "\n",
    "class_indices_mapping = train_gen.class_indices"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T19:56:36.771904Z",
     "start_time": "2024-03-19T19:56:36.597346Z"
    }
   },
   "id": "45de327b5135a9be",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def values(generator):\n",
    "    total_images = generator.n  \n",
    "    steps = total_images//batch_size \n",
    "    #iterations to cover all data, so if batch is 5, it will take total_images/5  iteration \n",
    "\n",
    "    x , y = [] , []\n",
    "    for i in range(steps):\n",
    "        a , b = generator.next()\n",
    "        x.extend(a) \n",
    "        y.extend(b)\n",
    "    return np.array(x), np.array(y)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T19:56:36.787860Z",
     "start_time": "2024-03-19T19:56:36.772876Z"
    }
   },
   "id": "5131b8e71c76c6a7",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Patches(L.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images = images,\n",
    "            sizes = [1, self.patch_size, self.patch_size, 1],\n",
    "            strides = [1, self.patch_size, self.patch_size, 1],\n",
    "            rates = [1, 1, 1, 1],\n",
    "            padding = 'VALID',\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T19:56:36.803820Z",
     "start_time": "2024-03-19T19:56:36.788834Z"
    }
   },
   "id": "8b6ca61865f50677",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 224 X 224\n",
      "Patch size: 7 X 7\n",
      "Patches per image: 1024\n",
      "Elements per patch: 147\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 400x400 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 400x400 with 1024 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "patch_size = 7  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "x = train_gen.next()\n",
    "image = x[0][0]\n",
    "\n",
    "\n",
    "\n",
    "resized_image = tf.image.resize(\n",
    "    tf.convert_to_tensor([image]), size = (image_size, image_size)\n",
    ")\n",
    "\n",
    "patches = Patches(patch_size)(resized_image)\n",
    "print(f'Image size: {image_size} X {image_size}')\n",
    "print(f'Patch size: {patch_size} X {patch_size}')\n",
    "print(f'Patches per image: {patches.shape[1]}')\n",
    "print(f'Elements per patch: {patches.shape[-1]}')\n",
    "\n",
    "n = int(np.sqrt(patches.shape[1]))\n",
    "plt.figure(figsize=(4, 4))\n",
    "\n",
    "for i, patch in enumerate(patches[0]):\n",
    "    ax = plt.subplot(n, n, i + 1)\n",
    "    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
    "    plt.imshow(patch_img.numpy().astype('uint8'))\n",
    "    plt.axis('off')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T19:56:57.867590Z",
     "start_time": "2024-03-19T19:56:36.804816Z"
    }
   },
   "id": "a62c75c409965d67",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from vit_keras import vit\n",
    "\n",
    "vit_model = vit.vit_b16(\n",
    "        image_size = image_size,\n",
    "        activation = 'softmax',\n",
    "        pretrained = True,\n",
    "        include_top = False,\n",
    "        pretrained_top = False,\n",
    "        classes = 5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T19:57:00.443338Z",
     "start_time": "2024-03-19T19:56:57.868562Z"
    }
   },
   "id": "e1778486eff1006b",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vision_transformer\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vit-b16 (Functional)        (None, 768)               85798656  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 768)               0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 768)              3072      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                49216     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 85,853,859\n",
      "Trainable params: 85,852,195\n",
      "Non-trainable params: 1,664\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "        vit_model,\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(64, activation = tf.keras.activations.gelu),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(32, activation = tf.keras.activations.gelu),\n",
    "        tf.keras.layers.Dense(16, activation = tf.keras.activations.gelu),\n",
    "        tf.keras.layers.Dense(3, 'softmax')\n",
    "    ],\n",
    "    name = 'vision_transformer')\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T19:57:01.509513Z",
     "start_time": "2024-03-19T19:57:00.446319Z"
    }
   },
   "id": "8ff0fc10eb56bd7e",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/20\n",
      "450/450 [==============================] - 137s 257ms/step - loss: 1.0254 - accuracy: 0.5829 - val_loss: 0.9764 - val_accuracy: 0.7688\n",
      "Epoch 2/20\n",
      " 73/450 [===>..........................] - ETA: 1:20 - loss: 0.7917 - accuracy: 0.8219"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "learning_rate = 1e-4\n",
    "\n",
    "optimizer = tfa.optimizers.RectifiedAdam(learning_rate = learning_rate)\n",
    "\n",
    "model.compile(optimizer = optimizer, \n",
    "              loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing = 0.2), \n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "STEP_SIZE_TRAIN = train_gen.n // train_gen.batch_size\n",
    "STEP_SIZE_VALID = valid_gen.n // valid_gen.batch_size\n",
    "\n",
    "\n",
    "\n",
    "early_stopping_callbacks = tf.keras.callbacks.EarlyStopping(patience = 4, restore_best_weights = True, verbose = 1)\n",
    "filepath = \"pt-miority-saved-model-{epoch:02d}.keras\"\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "History = model.fit(x = train_gen,\n",
    "          steps_per_epoch = STEP_SIZE_TRAIN,\n",
    "          validation_data = valid_gen,\n",
    "          validation_steps = STEP_SIZE_VALID,\n",
    "          epochs = 20,\n",
    "          callbacks = [early_stopping_callbacks, model_checkpoint_callback])\n",
    "\n",
    "model.save(\"smoted.keras\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-03-19T19:57:01.510499Z"
    }
   },
   "id": "b8b890de048e001f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "514bfe443c80d651"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
