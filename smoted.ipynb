{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import cv2\n",
    "from textwrap import wrap\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow_addons as tfa\n",
    "import os, warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b559ca3671f9fe46"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "image_size = 224\n",
    "batch_size = 16\n",
    "n_classes = 3\n",
    "EPOCHS = 30\n",
    "\n",
    "train_path = 'C:\\\\Users\\\\matus\\\\PycharmProjects\\\\VisionTransformer\\\\test_smote_images'\n",
    "\n",
    "classes = {0 : \"Dry\",\n",
    "           1 : \"Normal\",\n",
    "           2 : \"Wet\"}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ddef4113c26cc406"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def data_augment(image):\n",
    "    p_spatial = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n",
    "    p_rotate = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n",
    "    p_pixel_1 = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n",
    "    p_pixel_3 = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n",
    "    \n",
    "    # Flips\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    \n",
    "    if p_spatial > .75:\n",
    "        image = tf.image.transpose(image)\n",
    "        \n",
    "    # Rotates\n",
    "    if p_rotate > .75:\n",
    "        image = tf.image.rot90(image, k = 3) # rotate 270ยบ\n",
    "    elif p_rotate > .5:\n",
    "        image = tf.image.rot90(image, k = 2) # rotate 180ยบ\n",
    "    elif p_rotate > .25:\n",
    "        image = tf.image.rot90(image, k = 1) # rotate 90ยบ\n",
    "        \n",
    "    # Pixel-level transforms\n",
    "    if p_pixel_1 >= .4:\n",
    "        image = tf.image.random_saturation(image, lower = .7, upper = 1.3)\n",
    "    if p_pixel_3 >= .4:\n",
    "        image = tf.image.random_brightness(image, max_delta = .1)\n",
    "        \n",
    "    return image"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53774d413e2d58da"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255,\n",
    "                                                          samplewise_center = True,\n",
    "                                                          samplewise_std_normalization = True,\n",
    "                                                          validation_split = 0.2,\n",
    "                                                          preprocessing_function = data_augment)\n",
    "# set as training data\n",
    "\n",
    "train_gen  = datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size = batch_size,\n",
    "    seed = 1,\n",
    "    color_mode = 'rgb',\n",
    "    shuffle = True,\n",
    "    class_mode='categorical',\n",
    "    subset='training') \n",
    "\n",
    "# same directory as training data\n",
    "\n",
    "valid_gen  = datagen.flow_from_directory(\n",
    "    train_path ,\n",
    "    target_size=(224, 224),\n",
    "    batch_size = batch_size,\n",
    "    seed = 1,\n",
    "    color_mode = 'rgb',\n",
    "    shuffle = False,\n",
    "    class_mode='categorical',\n",
    "    subset='validation')\n",
    "\n",
    "class_indices_mapping = train_gen.class_indices"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a8b5bac39d09dfd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Patches(L.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images = images,\n",
    "            sizes = [1, self.patch_size, self.patch_size, 1],\n",
    "            strides = [1, self.patch_size, self.patch_size, 1],\n",
    "            rates = [1, 1, 1, 1],\n",
    "            padding = 'VALID',\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eafa56069d800d2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "batch_size = 16\n",
    "patch_size = 7  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "x = train_gen.next()\n",
    "image = x[0][0]\n",
    "\n",
    "\n",
    "\n",
    "resized_image = tf.image.resize(\n",
    "    tf.convert_to_tensor([image]), size = (image_size, image_size)\n",
    ")\n",
    "\n",
    "patches = Patches(patch_size)(resized_image)\n",
    "print(f'Image size: {image_size} X {image_size}')\n",
    "print(f'Patch size: {patch_size} X {patch_size}')\n",
    "print(f'Patches per image: {patches.shape[1]}')\n",
    "print(f'Elements per patch: {patches.shape[-1]}')\n",
    "\n",
    "n = int(np.sqrt(patches.shape[1]))\n",
    "plt.figure(figsize=(4, 4))\n",
    "\n",
    "for i, patch in enumerate(patches[0]):\n",
    "    ax = plt.subplot(n, n, i + 1)\n",
    "    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
    "    plt.imshow(patch_img.numpy().astype('uint8'))\n",
    "    plt.axis('off')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca8be21112cc39c3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from vit_keras import vit\n",
    "\n",
    "vit_model = vit.vit_b16(\n",
    "        image_size = image_size,\n",
    "        activation = 'softmax',\n",
    "        pretrained = True,\n",
    "        include_top = False,\n",
    "        pretrained_top = False,\n",
    "        classes = 5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c9b724edda1f48f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "        vit_model,\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(128, activation = tfa.activations.gelu),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(64, activation = tfa.activations.gelu),\n",
    "        tf.keras.layers.Dense(32, activation = tfa.activations.gelu),\n",
    "        tf.keras.layers.Dense(3, 'softmax')\n",
    "    ],\n",
    "    name = 'vision_transformer')\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc5488710467f214"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f5b5639d47101ac9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
